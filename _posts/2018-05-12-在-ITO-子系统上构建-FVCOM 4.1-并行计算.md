---
layout:     post
title:      在ITO子系统上构建FVCOM 4.1：并行计算
subtitle:   使用 fortran compiler 的并行计算，并测试 Estuary 算例
date:       2018-05-12
author:     DLXIII
header-img: img/post-bg-hacker.jpg
catalog: true
tags:
    - ito
    - fvcom
    - mpi
---


## 前言

在九州大学超级计算机ITO子系统A中，如何用英特尔编译器的MPI构建并执行FVCOM 4.1的备忘录。
请参考这里为系列的构建。
我们假设Series的构建环境已经到位。
源代码存在于fvcom4.1/FVCOM_source中，并将fvcom4.1/Examples/Estuary作为执行测试用例。

<!--more-->

## 准备使用英特尔编译器

加载。

~~~ bash
module load intel/2018
~~~

另外，您已经为英特尔C编译器设置了环境变量。
暂时用 ~/.bashrc设置它是件好事。
优化选项是超级计算机供应商的推荐值。
但是，由于优化选项必须使用METIS makefile进行设置，因此CFLAGS在此处不是必需的。

~~~bash
export CC=icc export CFLAGS="-O3 -no-prec-div -fp-model fast=2 -xHost"
export CPP="icc -E"
~~~

## METIS库的准备

准备系列中不需要的METIS库。
转到METIS源代码驻留的fvcom4.1/METIS_source，展开metis.tgz，然后继续构建。

~~~bash
tar xf metis.tgz
~~~

打开makefile并将第12行的MOPT设置为推荐值，如下所示。
只有-O3的默认值没有问题。
与环境变量不同，不要将选项用引号引起来。

~~~bash
            MOPT = -O3 -no-prec-div -fp-model fast=2 -xHost
~~~

makefile读取fvcom4.1的make.inc（它指定安装目标目录等）。
我们使用make.inc（FLAG）为Series设置。
但是，需要修改目录，如下所示。

~~~bash
include ../../FVCOM_source/make.inc
~~~

## METIS

~~~bash
make install
~~~

## 编辑make.inc

基于以前创建的系列的make.inc编辑（请参见此处）。
与系列唯一的区别是取消FLAG_4和RANLIB的注释。

~~~bash
            FLAG_1 =  -DDOUBLE_PRECISION
            FLAG_3 = -DWET_DRY
            FLAG_4 = -DMULTIPROCESSOR
            PARLIB = -lmetis  #-L/usr/local/lib -lmetis
            FLAG_8 =  -DLIMITED_NO
            FLAG_10  = -DGCN
            FLAG_14 = -DRIVER_FLOAT
~~~

注释英特尔编译器系列make.inc并按如下所示编辑MPI。
MPI的flat保留优化选项作为推荐值。

~~~bash
#--------------------------------------------------------------------------
#  Intel/MPI Compiler Definitions (ITO-A@kyushu-u)
#--------------------------------------------------------------------------
         CPP      = icc -E
         COMPILER = -DIFORT  
         CC       = mpiicc
         CXX      = mpiicpc
         CFLAGS   = -O3 -no-prec-div -fp-model fast=2 -xHost
         FC       = mpiifort
         DEBFLGS  = #-check all -traceback
         OPT      = -O3  -no-prec-div -fp-model fast=2 -xHost
#--------------------------------------------------------------------------
~~~

## 修改makefile

在系列中，我们从makefile中删除了mod_esmf_nesting.F以避免链接错误。
使用MPI，删除它并不重要，不删除它也没有问题。

## 测试用例河口算例

运行最基本的测试案例Estuary。 
Estuary的tst_run.nml有一个错误。
转到../Examples/Estuary/run，用编辑器打开tst_run.nml，并将河流数量修改为0，如下所示（最初为3）。

~~~bash
NML_RIVER_TYPE
RIVER_NUMBER = 0，
~~~

运行。
将可执行文件复制到../Examples/Estuary/run。
或链接在../Examples/Estuary/run中，执行如下。

~~~bash
./fvcom --casename = tst
~~~
或者

~~~bash
ln -s ../../../FVCOM_source/fvcom
~~~

运行

~~~bash
mpirun -np 2 ./fvcom --casename=tst
~~~

计算进度显示在屏幕上，计算完成后，TADA！应显示在最后。另外，包含计算结果的tst_0001.nc应该已经创建。

## 批处理

在../Examples/Estuary/run中，创建以下脚本（mpi.sh）。
它通过1节点36内核的36个进程来假设FLAT MPI。 
NUM_CORES指定每个节点的核心数量，NUM_PROCS指定节点数量乘以每个节点的核心数量。
详情请看这里。
https://www.cc.kyushu-u.ac.jp/scp/system/ITO/04-1_intel_compiler.html

~~~bash
#!/bin/bash
#PJM -L "rscunit=ito-a"
#PJM -L "rscgrp=ito-ss-dbg"
#PJM -L "vnode=1"
#PJM -L "vnode-core=36"
#PJM -L "elapse=10:00"
#PJM -j
#PJM -X

module load intel/2018

NUM_NODES={PJM_VNODES} 
NUM_CORES=36 
NUM_PROCS=36 
 
export I_MPI_PERHOST=NUM_CORES
export I_MPI_FABRICS=shm:ofa

export I_MPI_HYDRA_BOOTSTRAP=rsh
export I_MPI_HYDRA_BOOTSTRAP_EXEC=/bin/pjrsh
export I_MPI_HYDRA_HOST_FILE={PJM_O_NODEINF} 
 
mpiexec.hydra -n NUM_PROCS ./fvcom --casename=tst
~~~
